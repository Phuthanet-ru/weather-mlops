import tensorflow as tf
import mlflow
import mlflow.tensorflow
from tensorflow.keras import layers, models
import numpy as np
from PIL import Image
import os
from pathlib import Path

# üí° ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡πà‡∏≤ Remote Tracking URI (‡∏à‡∏≤‡∏Å Environment Variables)
# ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ô GitHub Actions
REMOTE_TRACKING_URI = os.environ.get("MLFLOW_TRACKING_URI")

# üí° ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ MLflow ‡πÉ‡∏ä‡πâ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏• Artifacts ‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠
# ‡∏™‡∏¥‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô 'Copy trained model' ‡πÉ‡∏ô main.yml ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏à‡∏≠
mlflow.set_tracking_uri(f"file:{Path.cwd()}/mlruns") 

ALLOWED_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp')

# ... (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô remove_dot_files ‡πÅ‡∏•‡∏∞ remove_corrupted_images ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ...
def remove_dot_files(root_dir):
    """‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏∞‡∏ö‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏†‡∏≤‡∏û"""
    count = 0
    if not os.path.isabs(root_dir):
        root_dir = os.path.join(os.getcwd(), root_dir)
    for root, dirs, files in os.walk(root_dir, topdown=False):
        for d in list(dirs):
            if d.startswith('.'):
                try:
                    os.rmdir(os.path.join(root, d))
                    dirs.remove(d)
                    count += 1
                except OSError:
                    pass
        for file in files:
            full_path = os.path.join(root, file)
            is_dot_file = file.startswith('.') or file.lower() in ['thumbs.db', '.ds_store', 'desktop.ini']
            is_invalid_image = not file.lower().endswith(ALLOWED_EXTENSIONS)
            if is_dot_file or is_invalid_image:
                try:
                    os.remove(full_path)
                    count += 1
                except Exception as e:
                    print(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå {full_path}: {e}")
    return count

def remove_corrupted_images(root_dir):
    """‡∏ï‡∏£‡∏ß‡∏à‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ (‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢)"""
    removed = 0
    for subdir, _, files in os.walk(root_dir):
        for file in files:
            path = os.path.join(subdir, file)
            if file.lower().endswith(ALLOWED_EXTENSIONS):
                try:
                    img = Image.open(path)
                    img.verify()
                except Exception:
                    print(f"üü• ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢: {path} ‚Üí ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å")
                    os.remove(path)
                    removed += 1
    return removed


def train_evaluate_register(preprocessing_run_id=None, epochs=10, lr=0.001):
    mlflow.set_experiment("Weather Classification - Model Training")

    # üí° ‡∏´‡∏≤‡∏Å‡∏°‡∏µ Remote URI ‡πÉ‡∏´‡πâ Log Metadata ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Remote Server ‡∏î‡πâ‡∏ß‡∏¢
    if REMOTE_TRACKING_URI:
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î URI ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô Remote ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Log Metadata (‡∏ä‡∏∑‡πà‡∏≠ Run, Metrics, Params)
        mlflow.set_tracking_uri(REMOTE_TRACKING_URI)
    
    # üí° ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ Artifact Path ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô main.yml
    ARTIFACT_PATH = "model" 

    with mlflow.start_run(run_name=f"cnn_lr_{lr}_ep_{epochs}"):
        mlflow.set_tag("ml.step", "model_training_evaluation")
        if preprocessing_run_id:
            mlflow.log_param("preprocessing_run_id", preprocessing_run_id)

        IMG_SIZE = (128, 128)
        BATCH_SIZE = 32
        data_path = "mlops_pipeline/data"
        
        # ... (‡πÇ‡∏Ñ‡πâ‡∏î‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•) ...

        cleaned_count = remove_dot_files(data_path)
        corrupted_count = remove_corrupted_images(data_path)
        print(f"üßº ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏∞‡∏ö‡∏ö {cleaned_count} ‡πÑ‡∏ü‡∏•‡πå, ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢ {corrupted_count} ‡πÑ‡∏ü‡∏•‡πå")

        print(f"üìÇ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å: {data_path}")
        temp_ds = tf.keras.preprocessing.image_dataset_from_directory(
            data_path, image_size=IMG_SIZE, batch_size=BATCH_SIZE)
        class_names = temp_ds.class_names

        if len(class_names) < 2:
            raise ValueError(f"‚ö†Ô∏è Dataset ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 classes ‡πÅ‡∏ï‡πà‡∏û‡∏ö‡πÄ‡∏û‡∏µ‡∏¢‡∏á {len(class_names)}: {class_names}")

        train_ds = tf.keras.preprocessing.image_dataset_from_directory(
            data_path, validation_split=0.2, subset="training", seed=42,
            image_size=IMG_SIZE, batch_size=BATCH_SIZE, labels='inferred', label_mode='int'
        )
        val_ds = tf.keras.preprocessing.image_dataset_from_directory(
            data_path, validation_split=0.2, subset="validation", seed=42,
            image_size=IMG_SIZE, batch_size=BATCH_SIZE, labels='inferred', label_mode='int'
        )

        data_augmentation = tf.keras.Sequential([
            layers.RandomFlip("horizontal"),
            layers.RandomRotation(0.1),
            layers.RandomZoom(0.1),
        ])

        model = models.Sequential([
            data_augmentation,
            layers.Rescaling(1./255),
            layers.Conv2D(32, (3,3), activation='relu'),
            layers.MaxPooling2D(),
            layers.Conv2D(64, (3,3), activation='relu'),
            layers.MaxPooling2D(),
            layers.Flatten(),
            layers.Dense(128, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(len(class_names), activation='softmax')
        ])

        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
                      loss='sparse_categorical_crossentropy', metrics=['accuracy'])

        history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)

        mlflow.log_param("epochs", epochs)
        mlflow.log_param("learning_rate", lr)
        mlflow.log_metric("train_accuracy", history.history["accuracy"][-1])
        mlflow.log_metric("val_accuracy", history.history["val_accuracy"][-1])

        # 1. Log Model (‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Artifacts ‡∏•‡∏á‡πÉ‡∏ô Local Disk)
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Å‡πà‡∏≠‡∏ô Register ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏ô‡∏î‡∏¥‡∏™‡∏Å‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Copy
        mlflow.tensorflow.log_model(
            model=model,    
            artifact_path=ARTIFACT_PATH,  # ‚úÖ ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ 'model'
            input_example=np.zeros((1, 128, 128, 3)),
            registered_model_name=None # ‚ùå ‡πÑ‡∏°‡πà‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        )

        # 2. Register Model (‡πÉ‡∏ä‡πâ URI ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)
        val_acc = history.history["val_accuracy"][-1]
        if val_acc >= 0.60:
            # ‡πÉ‡∏ä‡πâ URI ‡∏à‡∏≤‡∏Å Run ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ‡πÅ‡∏•‡∏∞ Artifact Path ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ
            run_id = mlflow.active_run().info.run_id
            
            # üí° ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Path ‡∏ó‡∏µ‡πà MLflow ‡∏°‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏∑‡∏≠ Local Path
            model_uri = f"runs:/{run_id}/{ARTIFACT_PATH}"
            print(f"üîó Registering model from URI: {model_uri}")
            
            # üí° ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ Artifact Path ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ 'model' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Run
            registered_model = mlflow.register_model(
                model_uri=model_uri,
                name="weather-classifier-prod"
            )
            print(f"‚úÖ Registered model version: {registered_model.version}")
        else:
            print(f"‚ö†Ô∏è Accuracy {val_acc:.2f} ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå ‡πÑ‡∏°‡πà‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•")

if __name__ == "__main__":
    train_evaluate_register()